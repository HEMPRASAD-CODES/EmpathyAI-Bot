{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1085454,"sourceType":"datasetVersion","datasetId":605165},{"sourceId":136560663,"sourceType":"kernelVersion"},{"sourceId":136584929,"sourceType":"kernelVersion"},{"sourceId":130383996,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# Quantum computing framework\n!pip install pennylane\n!pip install langchain\n\n# PyTorch ecosystem\n!pip install torch torchvision torchaudio\n\n# Hugging Face ecosystem\n!pip install transformers datasets accelerate\n\n# Fine-tuning and optimization libraries\n!pip install peft bitsandbytes trl\n\n# Hugging Face Hub\n!pip install huggingface_hub\n\n# Sentence transformers for embeddings\n!pip install sentence-transformers\n\n# Machine learning utilities\n!pip install scikit-learn\n\n# Numerical computing\n!pip install numpy\nimport torch\nimport gc\n\n# Clear GPU memory before starting\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n    # Force garbage collection\n    gc.collect()\nimport os\nimport re\nimport gc\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    pipeline as hf_pipeline\n)\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\nimport pennylane as qml\nfrom pennylane import numpy as qnp\nfrom pennylane.qnn import TorchLayer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom langchain.memory import ConversationBufferMemory\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n# Quantum Configuration\nN_QUBITS = 4\nQUANTUM_DEVICE = qml.device(\"default.qubit\", wires=N_QUBITS)\n\n# Emotion labels for classification\nEMOTION_LABELS = ['neutral', 'sadness', 'anxiety', 'anger', 'hopelessness', 'loneliness']\n\nclass QuantumEmotionClassifier(nn.Module):\n    \"\"\"Quantum-enhanced emotion classifier using PennyLane\"\"\"\n    \n    def __init__(self, n_qubits=N_QUBITS, n_classes=len(EMOTION_LABELS)):\n        super().__init__()\n        self.n_qubits = n_qubits\n        self.n_classes = n_classes\n        # Add emotion-to-prompt mapping\n        \n        # Classical preprocessing layers\n        self.classical_preprocessing = nn.Sequential(\n            nn.Linear(384, 64),  # From sentence transformer to reduced dimension\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(64, n_qubits),\n            nn.Tanh()  # Normalize to [-1, 1] for quantum encoding\n        )\n        \n        # Quantum layer\n        self.quantum_layer = self._create_quantum_layer()\n        \n        # Classical postprocessing\n        self.classical_postprocessing = nn.Sequential(\n            nn.Linear(n_qubits, 32),\n            nn.ReLU(),  \n            nn.Dropout(0.3),\n            nn.Linear(32, n_classes)\n        )\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        \"\"\"Initialize quantum circuit weights properly\"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                if module.bias is not None:\n                    nn.init.constant_(module.bias, 0)\n        \n    def _create_quantum_layer(self):\n        \"\"\"Create the quantum neural network layer\"\"\"\n        \n        def feature_map(x, wires):\n            \"\"\"Encode classical data into quantum states\"\"\"\n            for i in range(len(wires)):\n                qml.RY(x[i], wires=wires[i])\n                \n        def variational_circuit(weights, wires):\n            \"\"\"Parameterized quantum circuit for learning\"\"\"\n            # Layer 1: Single qubit rotations\n            for i in range(len(wires)):\n                qml.RY(weights[0, i], wires=wires[i])\n                qml.RZ(weights[1, i], wires=wires[i])\n            \n            # Layer 2: Entangling gates\n            for i in range(len(wires) - 1):\n                qml.CNOT(wires=[wires[i], wires[i+1]])\n            \n            # Layer 3: More rotations\n            for i in range(len(wires)):\n                qml.RY(weights[2, i], wires=wires[i])\n                \n        @qml.qnode(QUANTUM_DEVICE, interface=\"torch\")\n        def quantum_circuit(inputs, weights):\n            # Encode classical data\n            feature_map(inputs, wires=range(self.n_qubits))\n            \n            # Apply variational circuit\n            variational_circuit(weights, wires=range(self.n_qubits))\n            \n            # Measure expectation values\n            return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n        \n        # Fixed weight shapes - use proper tensor dimensions\n        weight_shapes = {\"weights\": [3, self.n_qubits]}\n        \n        return TorchLayer(quantum_circuit, weight_shapes)\n    \n    def forward(self, x):\n        x = self.classical_preprocessing(x)\n        x = self.quantum_layer(x)\n        x = x.reshape(-1, self.n_qubits)  # Ensure 2D shape\n        x = self.classical_postprocessing(x)\n        return torch.softmax(x, dim=1)\n\nclass EmotionDatasetGenerator:\n    \"\"\"Generate synthetic emotion dataset for training\"\"\"\n    \n    def __init__(self):\n        self.goemotions_path = goemotions_path\n        self.twitter_path = twitter_path\n        self.emotion_examples = {\n            'neutral': [\n                \"How are you today?\",\n                \"I went to the store.\",\n                \"The weather is nice.\",\n                \"I finished my work.\",\n                \"Let's have lunch.\"\n            ],\n            'sadness': [\n                \"I feel so down today.\",\n                \"Everything seems hopeless.\",\n                \"I can't stop crying.\",\n                \"I feel empty inside.\",\n                \"Nothing makes me happy anymore.\",\n                \"I'm feeling really low.\",\n                \"Life feels meaningless right now.\"\n            ],\n            'anxiety': [\n                \"I'm so worried about everything.\",\n                \"I can't stop feeling nervous.\",\n                \"My heart is racing with fear.\",\n                \"I'm panicking about the future.\",\n                \"I feel so stressed and overwhelmed.\",\n                \"I'm afraid something bad will happen.\",\n                \"I can't calm my anxious thoughts.\"\n            ],\n            'anger': [\n                \"I'm so frustrated right now.\",\n                \"Everything is making me angry.\",\n                \"I'm furious about this situation.\",\n                \"I feel so irritated and mad.\",\n                \"This is driving me crazy with rage.\",\n                \"I'm really upset and angry.\",\n                \"I can't control my anger anymore.\"\n            ],\n            'hopelessness': [\n                \"Nothing will ever get better.\",\n                \"I feel completely hopeless.\",\n                \"There's no point in trying anymore.\",\n                \"I've given up on everything.\",\n                \"I don't see any way out.\",\n                \"Everything feels pointless.\",\n                \"I have no hope left.\"\n            ],\n            'loneliness': [\n                \"I feel so alone in this world.\",\n                \"Nobody understands me.\",\n                \"I have no one to talk to.\",\n                \"I feel completely isolated.\",\n                \"Everyone has abandoned me.\",\n                \"I'm all by myself.\",\n                \"I feel so lonely and forgotten.\"\n            ]\n        }\n    def load_goemotions(path='/kaggle/input/goemotions-basic-eda'):\n        # Download from https://github.com/google-research/google-research/tree/master/goemotions/data\n        # Columns: 'text', 'labels'\n        df = pd.read_csv(path)\n        # GoEmotions has 27 emotions + neutral, multi-label per sample\n        # Map GoEmotions fine labels to your 6-class taxonomy\n        goemotions_map = {\n            'neutral': 'neutral',\n            'sadness': 'sadness',\n            'fear': 'anxiety',\n            'anger': 'anger',\n            'disgust': 'hopelessness',\n            'loneliness': 'loneliness',\n            # All other emotions â†’ you may map or ignore as needed\n        }\n        texts, labels = [], []\n        for _, row in df.iterrows():\n            label_list = eval(row['labels']) if isinstance(row['labels'], str) else row['labels']\n            for label in label_list:\n                mapped = goemotions_map.get(label)\n                if mapped:\n                    texts.append(row['text'])\n                    labels.append(mapped)\n                    break  # Use the first mapped label for this sample\n        return texts, labels\n    \n    def load_twitter_emotion(path='/kaggle/input/emotions-dataset-for-nlp'):\n        # Download from https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp\n        df = pd.read_csv(path)\n        twitter_map = {\n            'sadness': 'sadness',\n            'joy': 'neutral',  # Map joy to neutral or ignore\n            'love': 'neutral', # Map to neutral or ignore\n            'anger': 'anger',\n            'fear': 'anxiety',\n            'surprise': 'neutral', # Map to neutral or ignore\n        }\n        texts, labels = [], []\n        for _, row in df.iterrows():\n            mapped = twitter_map.get(row['emotion'])\n            if mapped:\n                texts.append(row['text'])\n                labels.append(mapped)\n        return texts, labels\n\n    def generate_dataset(self, samples_per_emotion=50):\n        \"\"\"Generate a balanced dataset of emotion examples\"\"\"\n        texts = []\n        labels = []\n        \n        for emotion, examples in self.emotion_examples.items():\n            # Repeat and slightly modify examples to reach desired sample count\n            current_examples = examples.copy()\n            while len(current_examples) < samples_per_emotion:\n                # Add variations of existing examples\n                base_examples = examples.copy()\n                for example in base_examples:\n                    if len(current_examples) >= samples_per_emotion:\n                        break\n                    # Simple variation by adding prefixes\n                    variations = [\n                        f\"I really {example.lower()}\",\n                        f\"Today {example.lower()}\",\n                        f\"Right now {example.lower()}\"\n                    ]\n                    current_examples.extend(variations)\n            \n            # Take exactly the number we need\n            current_examples = current_examples[:samples_per_emotion]\n            texts.extend(current_examples)\n            labels.extend([emotion] * len(current_examples))\n        go_texts, go_labels = load_goemotions(self.goemotions_path)\n        tw_texts, tw_labels = load_twitter_emotion(self.twitter_path)\n        texts += go_texts + tw_texts\n        labels += go_labels + tw_labels\n\n        # Balance the dataset for each emotion\n        df = pd.DataFrame({'text': texts, 'label': labels})\n        balanced = []\n        for emotion in EMOTION_LABELS:\n            group = df[df['label'] == emotion]\n            if len(group) > samples_per_emotion:\n                group = group.sample(samples_per_emotion, random_state=42)\n            balanced.append(group)\n        balanced_df = pd.concat(balanced)\n        return balanced_df['text'].tolist(), balanced_df['label'].tolist()\n\ndef get_device_config():\n    \"\"\"Check available hardware and return appropriate configuration\"\"\"\n    try:\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n            gpu_memory = torch.cuda.get_device_properties(0).total_memory\n            free_memory = torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)\n            \n            print(f\"CUDA available: {torch.cuda.get_device_name(0)}\")\n            print(f\"Total GPU memory: {gpu_memory / 1e9:.2f} GB\")\n            print(f\"Free GPU memory: {free_memory / 1e9:.2f} GB\")\n            \n            return {\n                \"device\": \"cuda\",\n                \"use_cuda\": True,\n                \"load_in_4bit\": free_memory > 1e9\n            }\n        else:\n            print(\"CUDA not available, using CPU\")\n            return {\n                \"device\": \"cpu\",\n                \"use_cuda\": False,\n                \"load_in_4bit\": False\n            }\n    except Exception as e:\n        print(f\"Device configuration error: {e}\")\n        return {\n            \"device\": \"cpu\",\n            \"use_cuda\": False,\n            \"load_in_4bit\": False\n        }\n\ndef load_classical_model(model_path=\"aboonaji/llama2finetune-v2\"):\n    \"\"\"Load the classical language model with optimizations\"\"\"\n    torch.cuda.empty_cache()\n    gc.collect()\n    try:\n        config = get_device_config()\n        hf_token = os.environ.get('HUGGINGFACE_TOKEN', None)\n        token_kwargs = {\"token\": hf_token} if hf_token else {}\n        \n        # Load tokenizer\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\n                model_path,\n                trust_remote_code=True,\n                max_length=512,\n                **token_kwargs\n            )\n            tokenizer.pad_token = tokenizer.eos_token\n            tokenizer.padding_side = \"right\"\n        except Exception as e:\n            print(f\"Error loading tokenizer: {e}\")\n            return None, None\n        \n        # Load model\n        try:\n            if config[\"use_cuda\"] and config[\"load_in_4bit\"]:\n                  quantization_config = BitsAndBytesConfig(\n                      load_in_4bit=True,\n                      bnb_4bit_compute_dtype=torch.float16,\n                      bnb_4bit_quant_type=\"nf4\",\n                      bnb_4bit_use_double_quant=True\n                  )\n                  \n                  model = AutoModelForCausalLM.from_pretrained(\n                      model_path,\n                      quantization_config=quantization_config,\n                      device_map=\"auto\",\n                      low_cpu_mem_usage=True,\n                      torch_dtype=torch.float16,\n                      max_memory={0: \"2GB\"},\n                      **token_kwargs\n                  )\n            else:\n                model = AutoModelForCausalLM.from_pretrained(\n                    model_path,\n                    device_map=config[\"device\"],\n                    torch_dtype=torch.float16 if config[\"use_cuda\"] else torch.float32,\n                    low_cpu_mem_usage=True,\n                    **token_kwargs\n                )\n            \n            return model, tokenizer\n            \n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            return None, None\n            \n    except Exception as e:\n        print(f\"Unexpected error in load_classical_model: {e}\")\n        return None, None\n\nclass QuantumEmotionalSupportBot:\n    \"\"\"Quantum-enhanced emotional support bot\"\"\"\n    \n    def __init__(self, classical_model=None, tokenizer=None, train_quantum=True):\n        self.classical_model = classical_model\n        self.tokenizer = tokenizer\n        self.is_dummy_mode = False\n        self.emotion_prompts = {\n            'neutral': \"Respond supportively while maintaining professionalism\",\n            'sadness': \"Acknowledge the sadness and offer compassionate support\",\n            'anxiety': \"Respond empathetically with reassurance and calm\",\n            'anger': \"Respond patiently to de-escalate tension\",\n            'hopelessness': \"Offer hope and emphasize their strength\",\n            'loneliness': \"Express understanding and emphasize connection\"\n        }\n        # Initialize sentence transformer for embeddings\n        print(\"Loading sentence transformer...\")\n        try:\n            self.sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n            print(\"Sentence transformer loaded successfully\")\n            if torch.cuda.is_available():\n              torch.cuda.empty_cache()\n              gc.collect()\n        except Exception as e:\n            print(f\"Error loading sentence transformer: {e}\")\n            self.sentence_encoder = None\n        \n        # Initialize quantum emotion classifier\n        print(\"Initializing quantum emotion classifier...\")\n        try:\n            self.quantum_classifier = QuantumEmotionClassifier()\n            self.label_encoder = LabelEncoder()\n            self.label_encoder.fit(EMOTION_LABELS)\n            print(\"Quantum emotion classifier initialized successfully\")\n        except Exception as e:\n            print(f\"Error initializing quantum classifier: {e}\")\n            raise e\n        \n        # Train quantum classifier if requested\n        if train_quantum and self.sentence_encoder:\n            try:\n                self.train_quantum_classifier()\n            except Exception as e:\n                print(f\"Error training quantum classifier: {e}\")\n                print(\"Continuing without quantum training...\")\n        \n        # Load classical model if not provided\n        if classical_model is None or tokenizer is None:\n            print(\"Loading classical language model...\")\n            self.classical_model, self.tokenizer = load_classical_model()\n            \n            if self.classical_model is None:\n                print(\"Classical model loading failed. Using dummy mode.\")\n                self._initialize_dummy_mode()\n                return\n        \n        # Initialize text generation pipeline\n        if not self.is_dummy_mode:\n            try:\n                config = get_device_config()\n                max_new_tokens = 128 if config[\"use_cuda\"] else 64\n                \n                self.text_generation_pipeline = hf_pipeline(\n                    task=\"text-generation\",\n                    model=self.classical_model,\n                    tokenizer=self.tokenizer,\n                    max_new_tokens=max_new_tokens,\n                    truncation=True,\n                    do_sample=True,\n                    temperature=0.7,\n                    top_p=0.9,\n                    repetition_penalty=1.2,\n                    device_map=\"auto\" if config[\"use_cuda\"] else None,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            except Exception as e:\n                print(f\"Error initializing pipeline: {e}\")\n                self._initialize_dummy_mode()\n                return\n        \n        self.memories = {}  # {user_id: ConversationBufferMemory}\n        self.user_emotions = {}\n        # Safety guardrails\n        self.crisis_keywords = [\"suicide\", \"kill myself\", \"end my life\", \"want to die\"]\n        self.crisis_response = (\n            \"I notice you've mentioned something that sounds serious. If you're having thoughts of \"\n            \"harming yourself, please know that you're not alone and support is available. \"\n            \"Please consider talking to a mental health professional or calling a crisis helpline:\\n\"\n            \"- National Suicide Prevention Lifeline: 988\\n\"\n            \"- Crisis Text Line: Text HOME to 741741\\n\\n\"\n            \"Would you like to talk more about what you're experiencing? I'm here to listen.\"\n        )\n        \n        print(\"Quantum EmotionalSupportBot initialized successfully!\")\n    def get_emotion_instruction(self, emotion, confidence):\n        base = self.emotion_prompts.get(emotion, \"\")\n        if confidence < 0.5:\n            return base + \" while acknowledging uncertainty\"\n        elif confidence > 0.8:\n            return base + \" with strong conviction\"\n        return base\n    def evaluate_emotion_classifier(model, data_loader, label_encoder, device='cpu'):\n        model.eval()\n        all_preds = []\n        all_labels = []\n        with torch.no_grad():\n            for inputs, labels in data_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                outputs = model(inputs)\n                preds = torch.argmax(outputs, dim=1)\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n    \n        # Metrics calculation\n        acc = accuracy_score(all_labels, all_preds)\n        precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n        recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        cm = confusion_matrix(all_labels, all_preds)\n        \n        print(\"Evaluation Metrics:\")\n        print(f\"Accuracy:  {acc:.4f}\")\n        print(f\"Precision: {precision:.4f}\")\n        print(f\"Recall:    {recall:.4f}\")\n        print(f\"F1-score:  {f1:.4f}\")\n        print(\"Confusion Matrix:\")\n        print(cm)\n        return {\n            \"accuracy\": acc,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_score\": f1,\n            \"confusion_matrix\": cm\n        }\n    def train_quantum_classifier(self, epochs=50, batch_size=32):\n        \"\"\"Train the quantum emotion classifier\"\"\"\n        print(\"Generating training data...\")\n        dataset_generator = EmotionDatasetGenerator(\n                goemotions_path='/kaggle/input/goemotions-basic-eda',\n                twitter_path='/kaggle/input/emotions-dataset-for-nlp',\n            )       \n        texts, labels = dataset_generator.generate_dataset(samples_per_emotion=200)\n        \n        print(f\"Generated {len(texts)} training samples\")\n        \n        # Encode texts using sentence transformer\n        print(\"Encoding texts...\")\n        try:\n            embeddings = self.sentence_encoder.encode(texts, convert_to_tensor=True)\n            print(f\"Embeddings shape: {embeddings.shape}\")\n        except Exception as e:\n            print(f\"Error encoding texts: {e}\")\n            return\n        \n        # Encode labels\n        encoded_labels = self.label_encoder.transform(labels)\n        labels_tensor = torch.tensor(encoded_labels, dtype=torch.long)\n        \n        # Split data\n        X_train, X_test, y_train, y_test = train_test_split(\n            embeddings, labels_tensor, test_size=0.2, random_state=42, stratify=encoded_labels\n        )\n        \n        # Get device and move everything to the same device\n        device = get_device_config()[\"device\"]\n        self.quantum_classifier.to(device)\n        \n        # Convert to proper device and ensure proper tensor types\n        X_train = X_train.to(device).float()\n        X_test = X_test.to(device).float()\n        y_train = y_train.to(device).long()\n        y_test = y_test.to(device).long()\n        \n        # Create data loaders\n        train_dataset = TensorDataset(X_train, y_train)\n        test_dataset = TensorDataset(X_test, y_test)\n        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n        \n        # Training setup\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.quantum_classifier.parameters(), lr=0.001)\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n        \n        print(f\"Training quantum classifier on {device}...\")\n        \n        # Training loop\n        for epoch in range(epochs):\n            self.quantum_classifier.train()\n            total_loss = 0\n            correct = 0\n            total = 0\n            \n            for batch_embeddings, batch_labels in train_loader:\n                # Ensure tensors are on the correct device\n                batch_embeddings = batch_embeddings.to(device).float()\n                batch_labels = batch_labels.to(device).long()\n                \n                optimizer.zero_grad()\n                outputs = self.quantum_classifier(batch_embeddings)\n                loss = criterion(outputs, batch_labels)\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += batch_labels.size(0)\n                correct += (predicted == batch_labels).sum().item()\n            \n            scheduler.step()\n            \n            # Validation\n            if (epoch + 1) % 10 == 0:\n                self.quantum_classifier.eval()\n                val_correct = 0\n                val_total = 0\n                with torch.no_grad():\n                    for batch_embeddings, batch_labels in test_loader:\n                        batch_embeddings = batch_embeddings.to(device).float()\n                        batch_labels = batch_labels.to(device).long()\n                        outputs = self.quantum_classifier(batch_embeddings)\n                        _, predicted = torch.max(outputs.data, 1)\n                        val_total += batch_labels.size(0)\n                        val_correct += (predicted == batch_labels).sum().item()\n                \n                print(f'Epoch [{epoch+1}/{epochs}], '\n                    f'Loss: {total_loss/len(train_loader):.4f}, '\n                    f'Train Acc: {100*correct/total:.2f}%, '\n                    f'Val Acc: {100*val_correct/val_total:.2f}%')\n        \n        print(\"Quantum classifier training completed!\")\n        # --- EVALUATION METRICS ---\n        metrics = self.evaluate_emotion_classifier(\n            model=self.quantum_classifier,\n            data_loader=test_loader,\n            label_encoder=self.label_encoder,\n            device=get_device_config()[\"device\"]\n        )\n        print(\"Final Evaluation Metrics:\")\n        for k, v in metrics.items():\n            print(f\"{k}: {v}\")\n        \n        # Save the trained model\n        try:\n            torch.save({\n                'model_state_dict': self.quantum_classifier.state_dict(),\n                'label_encoder': self.label_encoder\n            }, 'quantum_emotion_classifier.pth')\n            print(\"Quantum model saved to 'quantum_emotion_classifier.pth'\")\n        except Exception as e:\n            print(f\"Error saving quantum model: {e}\")\n    \n    def detect_emotion_quantum(self, text):\n        if self.sentence_encoder is None or not hasattr(self, 'quantum_classifier'):\n            return self.detect_emotion_classical(text)\n        try:\n            embedding = self.sentence_encoder.encode([text], convert_to_tensor=True)\n            if embedding.ndim == 1:\n                embedding = embedding.unsqueeze(0)\n            if embedding.shape[0] == 0 or embedding.shape[1] == 0:\n                return \"neutral\", 0.5\n    \n            device = next(self.quantum_classifier.parameters()).device\n            embedding = embedding.to(device)\n            self.quantum_classifier.eval()\n            with torch.no_grad():\n                prediction = self.quantum_classifier(embedding)\n                if prediction.ndim == 1:\n                    prediction = prediction.unsqueeze(0)\n                if prediction.shape[0] == 0 or prediction.shape[1] == 0:\n                    return \"neutral\", 0.5\n                predicted_indices = torch.argmax(prediction, dim=1).cpu().numpy()\n                if predicted_indices.size == 0:\n                    return \"neutral\", 0.5\n                try:\n                    predicted_class = int(predicted_indices[0])\n                except Exception:\n                    return \"neutral\", 0.5\n                confidence = float(torch.max(prediction).item())\n            emotion = self.label_encoder.inverse_transform([predicted_class])[0]\n            if confidence > 0.3:\n                return emotion, confidence\n            else:\n                return \"neutral\", confidence\n        except Exception as e:\n            # Optionally log the error for debugging\n            return self.detect_emotion_classical(text)\n\n\n\n    # Step 2: Use emotion history for more nuanced responses\n    def build_emotion_context(self, user_id):\n        \"\"\"Build emotional context from user's emotion history\"\"\"\n        if user_id not in self.user_emotions or not self.user_emotions[user_id]:\n            return \"\"\n        \n        recent_emotions = self.user_emotions[user_id][-3:]  # Last 3 emotions\n        emotion_context = []\n        \n        # Analyze emotion patterns\n        emotions_list = [e['emotion'] for e in recent_emotions]\n        confidences = [e['confidence'] for e in recent_emotions]\n        \n        # Check for persistent patterns\n        if len(set(emotions_list)) == 1 and emotions_list[0] != 'neutral':\n            emotion_context.append(f\"User has been consistently feeling {emotions_list[0]}\")\n        elif len(emotions_list) >= 2:\n            if emotions_list[-1] != emotions_list[-2]:\n                emotion_context.append(f\"User's emotion shifted from {emotions_list[-2]} to {emotions_list[-1]}\")\n        \n        # Check for intensity changes\n        if len(confidences) >= 2:\n            if confidences[-1] > confidences[-2] + 0.2:\n                emotion_context.append(\"Emotional intensity has increased\")\n            elif confidences[-1] < confidences[-2] - 0.2:\n                emotion_context.append(\"Emotional intensity has decreased\")\n        \n        # Check for concerning patterns\n        negative_emotions = ['sadness', 'anxiety', 'anger', 'hopelessness', 'loneliness']\n        if all(e in negative_emotions for e in emotions_list):\n            emotion_context.append(\"User has shown persistent negative emotions\")\n        \n        return \"; \".join(emotion_context) if emotion_context else \"\"\n\n    # Step 3: Reference previous topics mentioned\n    def extract_and_store_topics(self, user_input, user_id):\n        \"\"\"Extract and store key topics from user messages\"\"\"\n        if not hasattr(self, 'user_topics'):\n            self.user_topics = {}\n        \n        if user_id not in self.user_topics:\n            self.user_topics[user_id] = []\n        \n        # Simple topic extraction (could be enhanced with NLP)\n        topics = []\n        \n        # Personal references\n        personal_keywords = {\n            'work': ['work', 'job', 'career', 'office', 'boss', 'colleague', 'employment'],\n            'family': ['family', 'parents', 'mom', 'dad', 'mother', 'father', 'sibling', 'brother', 'sister'],\n            'relationship': ['boyfriend', 'girlfriend', 'partner', 'spouse', 'husband', 'wife', 'relationship', 'dating'],\n            'health': ['health', 'doctor', 'medicine', 'illness', 'pain', 'therapy', 'treatment'],\n            'school': ['school', 'college', 'university', 'student', 'exam', 'grade', 'homework', 'class'],\n            'friends': ['friend', 'friends', 'social', 'party', 'hangout', 'buddy'],\n            'money': ['money', 'financial', 'bills', 'debt', 'income', 'salary', 'budget', 'broke']\n        }\n        \n        user_input_lower = user_input.lower()\n        for topic, keywords in personal_keywords.items():\n            if any(keyword in user_input_lower for keyword in keywords):\n                topics.append(topic)\n        \n        # Store topics with timestamp\n        for topic in topics:\n            if topic not in [t['topic'] for t in self.user_topics[user_id][-5:]]:  # Avoid duplicates\n                self.user_topics[user_id].append({\n                    'topic': topic,\n                    'timestamp': len(self.user_topics[user_id]),\n                    'context': user_input[:100]  # Store snippet for context\n                })\n        \n        # Keep only recent topics\n        if len(self.user_topics[user_id]) > 10:\n            self.user_topics[user_id] = self.user_topics[user_id][-10:]\n    def get_smoothed_emotion(self, user_id, new_emotion, new_confidence):\n        history = self.user_emotions.get(user_id, [])\n        if history:\n            last = history[-1]\n            if new_confidence < 0.6 and last['confidence'] > 0.6:\n                return last['emotion'], last['confidence']\n        return new_emotion, new_confidence\n\n    def build_topic_context(self, user_id):\n        \"\"\"Build context from previously mentioned topics\"\"\"\n        # Check if user_topics attribute and user_id exist\n        if not hasattr(self, 'user_topics') or user_id not in self.user_topics:\n            return \"\"\n        \n        # Get the user's topic history\n        topic_history = self.user_topics[user_id]\n        if not topic_history:\n            return \"\"\n        \n        # Collect unique topics in reverse order (most recent first)\n        seen = set()\n        unique_recent_topics = []\n        for topic_entry in reversed(topic_history):\n            topic = topic_entry['topic']\n            if topic not in seen:\n                unique_recent_topics.append(topic)\n                seen.add(topic)\n            if len(unique_recent_topics) == 3:\n                break\n        \n        if not unique_recent_topics:\n            return \"\"\n        \n        # Reverse to maintain chronological order (oldest to newest)\n        unique_recent_topics.reverse()\n        return f\"Previous topics discussed: {', '.join(unique_recent_topics)}\"\n\n\n\n\n            \n    def generate_response_with_history(self, user_input, user_id=\"default_user\"):\n        \"\"\"Generate response with full conversation and emotional history\"\"\"\n        try:\n            # Safety check\n            safety_response = self.check_safety(user_input)\n            if safety_response:\n                return safety_response\n    \n            # Extract and store topics from current input\n            self.extract_and_store_topics(user_input, user_id)\n    \n            # Emotion detection\n            try:\n                if hasattr(self, 'quantum_classifier'):\n                    emotion, confidence = self.detect_emotion_quantum(user_input)\n                    #print(f\"Quantum emotion detection: {emotion} (confidence: {confidence:.3f})\")\n                else:\n                    emotion, confidence = self.detect_emotion_classical(user_input)\n                    #print(f\"Classical emotion detection: {emotion} (confidence: {confidence:.3f})\")\n            except Exception as e:\n                print(f\"Error in emotion detection: {e}\")\n                emotion, confidence = (\"neutral\", 0.5)\n            emotion, confidence = self.get_smoothed_emotion(user_id, emotion, confidence)\n\n            # Track emotion\n            self.track_emotion(user_id, emotion, confidence)\n    \n            # Build comprehensive context\n            from langchain.schema import HumanMessage, AIMessage\n            memory = self.get_memory(user_id)\n            chat_history = memory.load_memory_variables({})[\"chat_history\"]\n    \n            context_parts = []\n            for msg in chat_history[-5:]:\n                if isinstance(msg, HumanMessage):\n                    context_parts.append(f\"User: {msg.content}\")\n                elif isinstance(msg, AIMessage):\n                    context_parts.append(f\"Bot: {msg.content}\")\n    \n            # Emotional and topic context\n            emotion_context = self.build_emotion_context(user_id)\n            if emotion_context:\n                context_parts.append(f\"Emotional context: {emotion_context}\")\n            topic_context = self.build_topic_context(user_id)\n            if topic_context:\n                context_parts.append(topic_context)\n    \n            emotion_instruction = self.get_emotion_instruction(emotion, confidence)\n\n    \n            # Build the full prompt\n            if context_parts:\n                full_context = \"\\n\".join(context_parts)\n                formatted_input = (\n                    f\"<s>[INST] Context: {full_context}\\n\"\n                    f\"Emotional State: {emotion} (confidence: {confidence:.2f})\\n\"\n                    f\"Instruction: {emotion_instruction}\\n\"\n                    f\"Current message: {user_input} [/INST]\"\n                )\n            else:\n                formatted_input = (\n                    f\"<s>[INST] Emotional State: {emotion} (confidence: {confidence:.2f})\\n\"\n                    f\"Instruction: {emotion_instruction}\\n\"\n                    f\"Current message: {user_input} [/INST]\"\n                )\n    \n            # Generate response\n            try:\n                if self.is_dummy_mode:\n                    response = self.text_generation_pipeline(formatted_input)[0]['generated_text']\n                else:\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n                    with torch.no_grad():\n                        response = self.text_generation_pipeline(\n                            formatted_input,\n                            return_full_text=False,\n                            max_new_tokens=150,\n                            temperature=0.7,\n                            top_p=0.9\n                        )[0]['generated_text']\n            except Exception as e:\n                print(f\"Error in response generation: {e}\")\n                return \"I'm having trouble processing that right now. Could you rephrase or try again?\"\n    \n            # Clean and enhance response\n            clean_response = self.postprocess_response(response)\n            enhanced_response = self.enhance_response_with_history(clean_response, user_id, emotion)\n    \n            return enhanced_response\n    \n        except Exception as e:\n            print(f\"Critical error in generate_response_with_history: {e}\")\n            return \"I'm experiencing some technical difficulties. Could you share a bit more about what you're feeling?\"\n\n    def has_positive_shift(self, user_id):\n        history = self.user_emotions.get(user_id, [])\n        if len(history) >= 2:\n            prev, curr = history[-2], history[-1]\n            return prev['emotion'] != 'neutral' and curr['emotion'] == 'neutral' and curr['confidence'] > 0.6\n        return False\n\n    def enhance_response_with_history(self, response, user_id, current_emotion):\n        \"\"\"Add personalized touches based on conversation history\"\"\"\n        enhancements = []\n        \n        # Check for emotional progress\n        if user_id in self.user_emotions and len(self.user_emotions[user_id]) >= 2:\n            prev_emotion = self.user_emotions[user_id][-2]['emotion']\n            \n            if self.has_positive_shift(user_id):\n                enhancements.append(\"I'm glad to hear you're feeling a bit better than before.\")\n\n        # Reference recurring topics\n        if hasattr(self, 'user_topics') and user_id in self.user_topics:\n            recent_topics = [t['topic'] for t in self.user_topics[user_id][-3:]]\n            topic_counts = {}\n            for topic in recent_topics:\n                topic_counts[topic] = topic_counts.get(topic, 0) + 1\n            \n            recurring_topics = [topic for topic, count in topic_counts.items() if count >= 2]\n            if recurring_topics:\n                if 'work' in recurring_topics:\n                    enhancements.append(\"I know work has been on your mind lately.\")\n                elif 'family' in recurring_topics:\n                    enhancements.append(\"Family situations can be really complex.\")\n                elif 'relationship' in recurring_topics:\n                    enhancements.append(\"Relationships can bring up so many different feelings.\")\n        \n        # Check for persistent negative patterns\n        trend = self.get_emotion_trend(user_id)\n        if trend == \"persistent_negative\":\n            enhancements.append(\"I've noticed you've been going through a particularly challenging time. Please remember that support is available, and you don't have to go through this alone.\")\n        \n        # Combine response with enhancements\n        if enhancements:\n            return response + \" \" + \" \".join(enhancements)\n        \n        return response\n\n    # Additional helper method for topic analysis\n    def get_topic_summary(self, user_id):\n        \"\"\"Get a summary of topics the user has discussed\"\"\"\n        if not hasattr(self, 'user_topics') or user_id not in self.user_topics:\n            return \"No previous topics discussed\"\n        \n        topics = [t['topic'] for t in self.user_topics[user_id]]\n        topic_counts = {}\n        for topic in topics:\n            topic_counts[topic] = topic_counts.get(topic, 0) + 1\n        \n        # Sort by frequency\n        sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n        \n        summary = []\n        for topic, count in sorted_topics[:5]:  # Top 5 topics\n            summary.append(f\"{topic} ({count} times)\")\n        \n        return \"; \".join(summary)\n    def get_memory(self, user_id):\n        \"\"\"Get or create memory for a specific user\"\"\"\n        if user_id not in self.memories:\n            self.memories[user_id] = ConversationBufferMemory(\n                return_messages=True,\n                memory_key=\"chat_history\",\n                input_key=\"input\",\n                output_key=\"output\"\n            )\n        return self.memories[user_id]\n    # Enhanced debug info to show history context\n    def display_debug_info_enhanced(self, user_id=\"default_user\"):\n        \"\"\"Enhanced debug information including history analysis\"\"\"\n        print(\"\\n--- Quantum EmotionalSupportBot Enhanced Debug Info ---\")\n        \n        # Call original debug info\n        self.display_debug_info(user_id)\n        \n        # Additional history-aware debug info\n        print(\"=== CONVERSATION HISTORY ANALYSIS ===\")\n        \n        memory = self.get_memory(user_id)\n        chat_history = memory.load_memory_variables({})[\"chat_history\"]\n        print(f\"Message history ({len(chat_history)} messages):\")\n        for i, msg in enumerate(chat_history[-5:]):\n            content_preview = msg.content[:50] + '...' if len(msg.content) > 50 else msg.content\n            print(f\"  {i+1}. [{type(msg).__name__}] {content_preview}\")\n\n        # Emotion progression\n        if user_id in self.user_emotions:\n            emotions = [e['emotion'] for e in self.user_emotions[user_id]]\n            print(f\"Emotion progression: {' -> '.join(emotions[-5:])}\")\n        \n        # Topic analysis\n        if hasattr(self, 'user_topics') and user_id in self.user_topics:\n            topic_summary = self.get_topic_summary(user_id)\n            print(f\"Topics discussed: {topic_summary}\")\n        else:\n            print(\"No topics tracked yet\")\n        \n        # Context building preview\n        emotion_context = self.build_emotion_context(user_id)\n        topic_context = self.build_topic_context(user_id)\n        \n        if emotion_context:\n            print(f\"Current emotion context: {emotion_context}\")\n        if topic_context:\n            print(f\"Current topic context: {topic_context}\")\n        \n        print(\"====================================================\\n\")\n\n    # Update the main chat method to use the new history-aware generation\n    def chat_with_history(self, user_input, user_id=\"default_user\"):\n        \"\"\"Enhanced chat interface with full history awareness\"\"\"\n        try:\n            \n            # Clean input\n            cleaned_input = re.sub(r'\\[INST\\]|\\[/INST\\]', '', user_input).strip()\n            \n            # Generate response with history\n            response = self.generate_response_with_history(user_input, user_id)\n            \n            # Store user message\n            \n            memory = self.get_memory(user_id)\n            memory.save_context({\"input\": cleaned_input}, {\"output\": response})           \n            # Memory cleanup\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            return response\n            \n        except Exception as e:\n            print(f\"Error in chat_with_history method: {e}\")\n            return \"I apologize, but I'm having some technical difficulties. Let's try again.\"\n    \n    def detect_emotion_classical(self, text):\n        \"\"\"Fallback classical emotion detection\"\"\"\n        emotion_keywords = {\n            \"sadness\": [\"sad\", \"depressed\", \"unhappy\", \"miserable\", \"down\", \"blue\"],\n            \"anxiety\": [\"anxious\", \"worried\", \"nervous\", \"stressed\", \"panicking\", \"afraid\"],\n            \"anger\": [\"angry\", \"mad\", \"furious\", \"irritated\", \"annoyed\", \"frustrated\"],\n            \"hopelessness\": [\"hopeless\", \"pointless\", \"worthless\", \"empty\", \"meaningless\"],\n            \"loneliness\": [\"lonely\", \"alone\", \"isolated\", \"abandoned\", \"rejected\"]\n        }\n        \n        text_lower = text.lower()\n        for emotion, keywords in emotion_keywords.items():\n            if any(keyword in text_lower for keyword in keywords):\n                return (emotion, 0.7)  # Return as tuple\n        \n        return (\"neutral\", 0.5)  # Return as tuple\n    \n    def _initialize_dummy_mode(self):\n        \"\"\"Initialize dummy mode for when models fail to load\"\"\"\n        print(\"Initializing in dummy mode...\")\n        self.is_dummy_mode = True\n        \n        def dummy_generate(text, return_full_text=True, **kwargs):\n            text_lower = text.lower()\n            if \"sad\" in text_lower or \"down\" in text_lower:\n                response = \"I'm sorry to hear you're feeling down. Would you like to talk about what's contributing to these feelings?\"\n            elif \"anxious\" in text_lower or \"worried\" in text_lower:\n                response = \"Anxiety can be really challenging. Would it help to explore what's causing these feelings of worry?\"\n            elif \"angry\" in text_lower or \"frustrated\" in text_lower:\n                response = \"I can understand feeling frustrated. Sometimes anger points to something important to us.\"\n            elif \"alone\" in text_lower or \"lonely\" in text_lower:\n                response = \"Feeling alone can be really difficult. Connection is so important for our wellbeing.\"\n            else:\n                response = \"Thank you for sharing that with me. Would you like to tell me more about what you're experiencing?\"\n            \n            if return_full_text:\n                return [{\"generated_text\": text + \" [/INST] \" + response}]\n            else:\n                return [{\"generated_text\": response}]\n        \n        self.text_generation_pipeline = dummy_generate\n    \n    def check_safety(self, user_input):\n        \"\"\"Check for crisis situations\"\"\"\n        input_lower = user_input.lower()\n        for keyword in self.crisis_keywords:\n            if keyword in input_lower:\n                return self.crisis_response\n        return None\n    \n    def track_emotion(self, user_id, emotion, confidence):\n        \"\"\"Track emotions with confidence scores\"\"\"\n        if user_id not in self.user_emotions:\n            self.user_emotions[user_id] = []\n        \n        self.user_emotions[user_id].append({\n            'emotion': emotion,\n            'confidence': confidence,\n            'timestamp': len(self.user_emotions[user_id])\n        })\n        \n        # Keep only recent emotions\n        if len(self.user_emotions[user_id]) > 5:\n            self.user_emotions[user_id] = self.user_emotions[user_id][-5:]\n    \n    def get_emotion_trend(self, user_id):\n        \"\"\"Analyze emotion trends\"\"\"\n        if user_id not in self.user_emotions or len(self.user_emotions[user_id]) < 3:\n            return None\n        \n        recent_emotions = [e['emotion'] for e in self.user_emotions[user_id][-3:]]\n        \n        # Check for persistent negative emotions\n        negative_emotions = ['sadness', 'anxiety', 'anger', 'hopelessness', 'loneliness']\n        persistent_negative = all(e in negative_emotions for e in recent_emotions)\n        \n        if persistent_negative:\n            return f\"persistent_negative\"\n        \n        # Check for improvement\n        if recent_emotions[-1] == \"neutral\" and any(e != \"neutral\" for e in recent_emotions[:-1]):\n            return \"improving\"\n        \n        return None\n    \n    def preprocess_input(self, user_input):\n        \"\"\"Format user input with instruction tags\"\"\"\n        clean_input = re.sub(r'\\[INST\\]|\\[/INST\\]', '', user_input).strip()\n        return f\"<s>[INST] {clean_input} [/INST]\"\n    \n    def postprocess_response(self, response_text):\n        \"\"\"Clean up model response\"\"\"\n        try:\n            parts = response_text.split('[/INST]')\n            if len(parts) > 1:\n                response = parts[-1].strip()\n            else:\n                response = response_text.strip()\n            \n            response = response.replace('<s>', '').replace('</s>', '').strip()\n            return response\n        except Exception as e:\n            print(f\"Error in postprocess_response: {e}\")\n            return response_text.replace('[/INST]', '').replace('[INST]', '').strip()\n    \n    def generate_response(self, user_input, user_id=\"default_user\"):\n        \"\"\"Generate response with quantum emotion awareness\"\"\"\n        try:\n            # Safety check\n            safety_response = self.check_safety(user_input)\n            if safety_response:\n                return safety_response\n            \n            # Quantum emotion detection with proper error handling\n            try:\n                if hasattr(self, 'quantum_classifier'):\n                    emotion, confidence = self.detect_emotion_quantum(user_input)\n                    #print(f\"Quantum emotion detection: {emotion} (confidence: {confidence:.3f})\")\n                else:\n                    emotion, confidence = self.detect_emotion_classical(user_input)\n                    #print(f\"Classical emotion detection: {emotion} (confidence: {confidence:.3f})\")\n            except Exception as e:\n                print(f\"Error in emotion detection: {e}\")\n                emotion, confidence = (\"neutral\", 0.5)\n            emotion, confidence = self.get_smoothed_emotion(user_id, emotion, confidence)\n\n            # Track emotion\n            self.track_emotion(user_id, emotion, confidence)\n            \n            # Format input\n            formatted_input = self.preprocess_input(user_input)\n            \n            # Generate response with unified error handling\n            try:\n                if self.is_dummy_mode:\n                    response = self.text_generation_pipeline(formatted_input)[0]['generated_text']\n                else:\n                    if torch.cuda.is_available():\n                        torch.cuda.empty_cache()\n                    with torch.no_grad():\n                        response = self.text_generation_pipeline(\n                            formatted_input,\n                            return_full_text=False\n                        )[0]['generated_text']\n                            \n            except Exception as e:\n                print(f\"Error in response generation: {e}\")\n                return \"I'm having trouble processing that right now. Could you rephrase or try again?\"\n            \n            # Clean response\n            try:\n                clean_response = self.postprocess_response(response)\n            except Exception as e:\n                print(f\"Error in response postprocessing: {e}\")\n                clean_response = \"I'm having some trouble with my response processing. Could you try again?\"\n            \n            # Add emotion-aware enhancements\n            try:\n                trend = self.get_emotion_trend(user_id)\n                if trend == \"persistent_negative\":\n                    clean_response += \"\\n\\nI've noticed you've been going through a difficult time. Have you considered speaking with a mental health professional who might provide additional support?\"\n            except Exception as e:\n                print(f\"Error in trend analysis: {e}\")\n            \n            return clean_response\n            \n        except Exception as e:\n            print(f\"Critical error in generate_response: {e}\")\n            return \"I'm experiencing some technical difficulties. Could you share a bit more about what you're feeling?\"\n\n    \n    def chat(self, user_input, user_id=\"default_user\"):\n        \"\"\"Main chat interface\"\"\"\n        try:\n            \n            \n            # Clean input\n            cleaned_input = re.sub(r'\\[INST\\]|\\[/INST\\]', '', user_input).strip()\n            \n            # Generate response\n            response = self.generate_response(user_input, user_id)\n            \n            self.memory.save_context({\"input\": cleaned_input}, {\"output\": response})\n\n            \n            # Memory cleanup\n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            return response\n            \n        except Exception as e:\n            print(f\"Error in chat method: {e}\")\n            return \"I apologize, but I'm having some technical difficulties. Let's try again.\"\n    \n    def display_debug_info(self, user_id=\"default_user\"):\n        \"\"\"Display debug information including quantum states\"\"\"\n        print(\"\\n--- Quantum EmotionalSupportBot Debug Info ---\")\n        \n        # System info\n        device_config = get_device_config()\n        print(f\"Device: {device_config['device']}\")\n        if device_config['use_cuda']:\n            print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n        \n        print(f\"Running in {'dummy' if self.is_dummy_mode else 'normal'} mode\")\n        print(f\"Quantum classifier available: {hasattr(self, 'quantum_classifier')}\")\n        print(f\"Sentence encoder available: {self.sentence_encoder is not None}\")\n        \n        # Emotion tracking\n        if user_id in self.user_emotions:\n            print(f\"\\nEmotion history for {user_id}:\")\n            for i, emotion_data in enumerate(self.user_emotions[user_id]):\n                print(f\"  {i+1}. {emotion_data['emotion']} (confidence: {emotion_data['confidence']:.3f})\")\n            \n            trend = self.get_emotion_trend(user_id)\n            if trend:\n                print(f\"Emotional trend: {trend}\")\n        else:\n            print(\"No emotion data available\")\n        \n        chat_history = self.memory.load_memory_variables({})[\"chat_history\"]\n        print(f\"Message history ({len(chat_history)} messages):\")\n        for i, msg in enumerate(chat_history[-5:]):\n            print(f\"  {i+1}. {msg.content[:50]}...\")\n        \n        print(\"---------------------------------------------\\n\")\n    \n    def cleanup(self):\n        \"\"\"Clean up resources\"\"\"\n        try:\n            if hasattr(self, 'classical_model') and self.classical_model and not self.is_dummy_mode:\n                if hasattr(self, 'text_generation_pipeline'):\n                    del self.text_generation_pipeline\n                \n                if torch.cuda.is_available():\n                    self.classical_model.to('cpu')\n                \n                del self.classical_model\n                self.classical_model = None\n            \n            if hasattr(self, 'quantum_classifier'):\n                if torch.cuda.is_available():\n                    self.quantum_classifier.to('cpu')\n            \n            # Clear memory\n            self.memory.clear()\n\n            self.user_emotions = {}\n            \n            gc.collect()\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            \n            print(\"Resources cleaned up successfully\")\n        except Exception as e:\n            print(f\"Error during cleanup: {e}\")\n\ndef initialize_quantum_bot(train_quantum=True):\n    \"\"\"Initialize the quantum-enhanced emotional support bot\"\"\"\n    try:\n        print(\"Initializing Quantum EmotionalSupportBot...\")\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        bot = QuantumEmotionalSupportBot(train_quantum=train_quantum)\n        return bot\n    except Exception as e:\n        print(f\"Failed to initialize Quantum EmotionalSupportBot: {e}\")\n        return None\n\ndef run_quantum_interactive_session():\n    \"\"\"Run interactive session with quantum-enhanced bot\"\"\"\n    # Initialize bot\n    bot = initialize_quantum_bot(train_quantum=True)\n    \n    if not bot:\n        print(\"Could not initialize bot. Exiting.\")\n        return\n    \n    print(\"\\n=== Quantum EmotionalSupportBot Interactive Session ===\")\n    print(\"This bot uses quantum computing for enhanced emotion detection!\")\n    print(\"Type your messages and the bot will respond with quantum-enhanced empathy.\")\n    print(\"\\nSpecial commands:\")\n    print(\"  /debug   - Show debug information including quantum states\")\n    print(\"  /reset   - Reset conversation history\")\n    print(\"  /quantum - Show quantum classifier info\")\n    print(\"  /exit    - End session\")\n    print(\"========================================================\\n\")\n    \n    user_id = \"quantum_user\"\n    \n\n    while True:\n        try:\n            # Get user input\n            user_input = input(\"You: \").strip()\n\n            # Check for special commands\n            if user_input.lower() == \"/exit\":\n                print(\"Ending session. Take care!\")\n                # Cleanup before exit\n                if hasattr(bot, 'cleanup'):\n                    bot.cleanup()\n                break\n            elif user_input.lower() == \"/debug\":\n                bot.display_debug_info(user_id)\n                continue\n            elif user_input.lower() == \"/clean\":\n                # Force memory cleanup\n                if hasattr(bot, 'cleanup'):\n                    bot.cleanup()\n                    print(\"Memory cleaned. Reinitializing...\")\n                    bot = initialize_quantum_bot(use_dummy_if_failed=True)\n                    if not bot:\n                        print(\"Failed to reinitialize. Exiting.\")\n                        break\n                continue\n            elif user_input.lower() == \"/reset\":\n                    # Get user-specific memory\n                    memory = bot.get_memory(user_id)\n                    memory.clear()\n                    if user_id in bot.user_emotions:\n                        bot.user_emotions[user_id] = []\n                    print(\"Conversation history and emotion tracking have been reset.\")\n\n            elif not user_input:\n                continue\n\n            # Process the input and get response\n            response = bot.chat_with_history(user_input, user_id)\n\n            # Display the response\n            print(f\"Bot: {response}\")\n\n            # Show current emotion after each exchange\n            if user_id in bot.user_emotions and bot.user_emotions[user_id]:\n                current_emotion = bot.user_emotions[user_id][-1]\n                #if current_emotion != \"neutral\":\n                    #print(f\"[Detected emotion: {current_emotion}]\")\n\n            # Periodically check memory status\n            if torch.cuda.is_available() and hasattr(bot, 'model') and bot.classical_model and not bot.is_dummy_mode:\n                try:\n                    free_memory = torch.cuda.memory_reserved(0) - torch.cuda.memory_allocated(0)\n                    if free_memory < 500e6:  # Less than 500MB free\n                        print(\"[Warning: Low GPU memory. Consider using /clean to free up resources]\")\n                except:\n                    pass  # Ignore errors in memory checking\n\n        except KeyboardInterrupt:\n            print(\"\\nSession interrupted. Ending session.\")\n            if hasattr(bot, 'cleanup'):\n                bot.cleanup()\n            break\n        except Exception as e:\n            print(f\"Error in interactive session: {e}\")\n            print(\"Let's continue anyway.\")\n\n# Main execution\nif __name__ == \"__main__\":\n    try:\n        run_quantum_interactive_session()\n    finally:\n        # Final cleanup before exit\n        print(\"Cleaning up resources...\")\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T13:13:17.860571Z","iopub.execute_input":"2025-06-16T13:13:17.860846Z"}},"outputs":[{"name":"stdout","text":"Collecting pennylane\n  Downloading PennyLane-0.41.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.15.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.4.2)\nCollecting rustworkx>=0.14.0 (from pennylane)\n  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.7.0)\nCollecting tomlkit (from pennylane)\n  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\nCollecting appdirs (from pennylane)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nCollecting autoray>=0.6.11 (from pennylane)\n  Downloading autoray-0.7.1-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\nCollecting pennylane-lightning>=0.41 (from pennylane)\n  Downloading pennylane_lightning-0.41.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.13.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (25.0)\nCollecting diastatic-malt (from pennylane)\n  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\nCollecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.41->pennylane)\n  Downloading scipy_openblas32-0.3.29.265.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (56 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->pennylane) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->pennylane) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->pennylane) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->pennylane) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->pennylane) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->pennylane) (2.4.1)\nRequirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\nRequirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (3.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.4.26)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\nRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pennylane) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->pennylane) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->pennylane) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->pennylane) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->pennylane) (2024.2.0)\nDownloading PennyLane-0.41.1-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading autoray-0.7.1-py3-none-any.whl (930 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pennylane_lightning-0.41.1-cp311-cp311-manylinux_2_28_x86_64.whl (2.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\nDownloading scipy_openblas32-0.3.29.265.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: appdirs, tomlkit, scipy-openblas32, autoray, diastatic-malt, rustworkx, pennylane-lightning, pennylane\nSuccessfully installed appdirs-1.4.4 autoray-0.7.1 diastatic-malt-2.15.2 pennylane-0.41.1 pennylane-lightning-0.41.1 rustworkx-0.16.0 scipy-openblas32-0.3.29.265.1 tomlkit-0.13.3\nRequirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.22)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.50)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\nRequirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.23)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (9.1.2)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.49->langchain)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain) (4.13.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain) (3.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\nDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed packaging-24.2\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nCollecting trl\n  Downloading trl-0.18.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.31.1)\nRequirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.6.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.0.0->trl) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.11.18)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2025.4.26)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->peft) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\nDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.18.2-py3-none-any.whl (366 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m366.4/366.4 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: trl, bitsandbytes\n","output_type":"stream"}],"execution_count":null}]}